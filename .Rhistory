theme(axis.text=element_text(size=6))+
theme(strip.text.x=element_text(size=6),legend.position="bottom")
corplot.avgdisc
## save plots
if(exists("datafilter")){
ggsave(file=paste0(figfolder,"validation_correlations_avgdisc-",modelname,"_",datafilter,".pdf"),
width=6.5,height=4,corplot.avgdisc)
}else{
ggsave(file=paste0(figfolder,"validation_correlations_avgdisc-",modelname,".pdf"),
width=6.5,height=2,corplot.avgdisc)
}
if(exists("datafilter")){
ggsave(file=paste0(figfolder,"validation_correlations-",modelname,"_",datafilter,".pdf"),
width=6.5,height=9,corplot)
}else{
ggsave(file=paste0(figfolder,"validation_correlations-",modelname,".pdf"),
width=6.5,height=9,corplot)
}
# validation2: compare estimates with Bergquist and Warshaw US dataset ####
## national time series comparison ####
bw.nat<-readstata13::read.dta13("predictors/bergquistwarshaw_national.dta")%>%
filter(year>=2002)%>%
mutate(year2=case_when(
year>2001&year<=2003~"2002-03",
year>2003&year<=2005~"2004-05",
year>2005&year<=2007~"2006-07",
year>2007&year<=2009~"2008-09",
year>2009&year<=2011~"2010-11",
year>2011&year<=2013~"2012-13",
year>2013&year<=2015~"2014-15",
year>2015&year<=2017~"2016-17",
year>2017&year<=2019~"2018-19",
year>2019&year<=2021~"2020-21",
year>2021&year<=2023~"2022-23"))%>%
group_by(year2)%>%
summarise(climate_concern_median=quantile(mass_climate_concern,probs=0.5,na.rm=TRUE))
us.nat<-est.nat%>%
filter(iso_3166=="US")
table(us.nat$year)
us.nat%<>%left_join(bw.nat,by=c("year"="year2"))
cor(us.nat$mean,us.nat$climate_concern_median,use="complete.obs") ## 0.76
### Figure S7: time-series plot with national data ####
usnatplot<-ggplot(us.nat,aes(x=mean,y=climate_concern_median,label=year))+
geom_text()+
annotate(geom="text",x=1.1,y=-.76,label=round(cor(us.nat$mean,us.nat$climate_concern_median,use="complete.obs"),2),
color="blue")+
labs(x="Global climate concern (US)",y="US climate concern\n(Bergquist and Warshaw 2019)")+
geom_smooth(method="lm",se=FALSE)+
scale_x_continuous(limits=c(0.9,1.2))+
theme_bw()
usnatplot
ggsave(filename=paste0(figfolder,"validation_correlations_nationaltimeseries-",modelname,datafilter,".pdf"),
width=6.5,height=3.5,usnatplot)
## sub-national correlations with Bergquist and Warshaw data ####
us<-est.reg%>%
filter(iso_3166=="US")%>%
separate(mergekey,into=c("us","abb"),sep="-")
# us<-us%>%
#   separate(mergekey,into=c("us","abb"),sep="-")
bw<-read_csv("predictors/bergquistwarshaw_climate.csv")
table(bw$period)
table(us$year)
## for time-collapsed data
if(exists("timecollapse")){
if(timecollapse=="5yr"){
bw<-bw%>%mutate(
year2=case_when(period<=2004~"1998-2004",
period>2004&period<=2009~"2005-2009",
period>2009&period<=2014~"2010-2014",
period>2014&period<=2021~"2015-2021"))%>%
group_by(year2,abb)%>%
summarise(climate=mean(climate,na.rm=TRUE))%>%
ungroup()
}else if(timecollapse=="2yr"){
bw<-bw%>%mutate(
year2=case_when(#period<=1999~"1998-99",
#period>1999&period<=2001~"2000-01",
# period<=2003~"1998-2003",
period>2001&period<=2003~"2002-03",
period>2003&period<=2005~"2004-05",
period>2005&period<=2007~"2006-07",
period>2007&period<=2009~"2008-09",
period>2009&period<=2011~"2010-11",
period>2011&period<=2013~"2012-13",
period>2013&period<=2015~"2014-15",
period>2015&period<=2017~"2016-17",
period>2017&period<=2019~"2018-19",
period>2019&period<=2021~"2020-21",
period>2021&period<=2023~"2022-23"))%>%
group_by(year2,abb)%>%
summarise(climate=quantile(climate,probs=0.5,na.rm=TRUE))%>%
ungroup()
}
}
us<-left_join(us,bw,by=c("abb","year"="year2"))
## regress state-level global estimates on PBCW estimates, with state FEs ####
library(fixest)
statemod<-lm(mean.scl~climate+factor(abb),data=us)
partial_r2(statemod,covariates="climate") ## 0.17
cordataus<-us%>%
filter(!is.na(climate))%>%
group_by(year)%>% ## year2? 220813
summarize(corr=round(cor(mean,climate,use="complete.obs"),2))%>%
arrange(desc(corr))
us<-left_join(us,cordataus,by="year") ## year2? 220813
### figure S7: Convergent validation (cross-sectional) with Bergquist and Warshaw state-level data ####
uscorplot<-us%>%
filter(!is.na(climate))%>%
ggplot(aes(x=mean,y=climate,label=abb))+
geom_text(size=2)+
geom_text(data=us[!is.na(us$climate),],aes(x=1,y=4,label=corr),color="blue")+
facet_wrap(~year,nrow=6,ncol=4)+
labs(x="Global climate concern",y="Bergquist and Warshaw\nclimate concern")+
theme_bw()+
theme(strip.text.x=element_text(size=8),legend.position="bottom")
uscorplot ## slightly better correlations here with oecd only and noqfilter(?)
if(exists("datafilter")){
ggsave(file=paste0(figfolder,"validation_correlations_statelevel-",modelname,"_",datafilter,"_",".pdf"),
width=6.5,height=3.5,uscorplot)
}else{
ggsave(file=paste0(figfolder,"validation_correlations_statelevel-",modelname,"_",".pdf"),
width=6.5,height=3.5,uscorplot)
}
timeseries<-d.val%>%
select(source2,year2,iso_3166,question)%>%
distinct()%>%
mutate(yearquestion=paste(year2,question,sep="-"))%>%
select(question,yearquestion)%>%
distinct()%>%
group_by(question)%>%
summarise(n=n())%>%
arrange(desc(n))
quantile(timeseries$n)
## select questions that are asked in 3+ years
sel<-timeseries[timeseries$n>=3,"question"]
sel<-bind_rows(sel,sel.np)
d%<>%
filter(question%in%sel$question)
d.yesresponses<-survey.data%>%
select(all_of(sel$question),year2,iso_3166)%>%
mutate_at(sel$question,as.numeric)%>%
group_by(iso_3166,year2)%>%
summarise_at(sel$question,~mean((.),na.rm=TRUE))%>%
ungroup()
## select questions that are asked in 3+ years
sel<-timeseries[timeseries$n>=3,"question"]
d%<>%
filter(question%in%sel$question)
d.yesresponses<-survey.data%>%
select(all_of(sel$question),year2,iso_3166)%>%
mutate_at(sel$question,as.numeric)%>%
group_by(iso_3166,year2)%>%
summarise_at(sel$question,~mean((.),na.rm=TRUE))%>%
ungroup()
## count all non-NA values for each country-year-question
d.samples<-survey.data%>%
select(all_of(sel$question),year2,iso_3166)%>%
mutate_at(sel$question,as.numeric)%>%
group_by(iso_3166,year2)%>%
summarise_at(vars(sel$question),funs(n=sum(!is.na(.))))%>%
ungroup()
d.samples<-d.samples%>%
pivot_longer(cols=3:ncol(d.samples),names_to="question",values_to="size")%>%
mutate(question=substr(question,1,nchar(question)-2))
## combine into single dataset and get proportion of yes responses
d.yesresponses<-d.yesresponses%>%
pivot_longer(cols=sel$question,names_to='question',values_to='yes')
d.val<-full_join(d.yesresponses,d.samples,by=c("iso_3166","question","year2"))
d.val<-d.val%>%
filter(size!=0)%>%
bind_rows(d.val.np2)
country.est<-country.poststrat%>%select(iso_3166,year2,mean)## year2? 220813
d.val<-d.val%>%
left_join(country.est,by=c("iso_3166","year2")) ## year2? 220813
d.val<-d.val%>%left_join(timeseries,by="question")
dwide<-d.val%>%
pivot_wider(names_from="question",values_from="yes")%>%
mutate_at(c(6:ncol(.)),~scale(.))%>% ## standardize these columns
pivot_longer(cols=c(6:ncol(.)),names_to="question",values_to="yes.scl")
d.val<-d.val%>%left_join(dwide,by=intersect(names(d.val),names(dwide)))
## correlation across time series ####
dwide<-d.val%>%
select(iso_3166,year2,question,yes.scl)%>%
pivot_wider(names_from="question",values_from="yes.scl")
## pairwise combinations:
combos<-combn(names(dwide)[3:ncol(dwide)],2)
r2s<-list()
for(i in 1:ncol(combos)){
mod.data<-dwide%>% ## country-years for which we have data from both questions
select(iso_3166,year2,combos[1,i],combos[2,i])%>%
na.omit()
# print(nrow(mod.data))
if(nrow(mod.data)>length(unique(mod.data$iso_3166))){ ## filter to combos with enough degrees of freedom to estimate regression
print(paste("i=",i))
# print(combos[,i])
# print(paste("country year combos=",max(table(mod.data$iso_3166))))
mod<-lm(unlist(mod.data[,3])~unlist(mod.data[,4])+factor(mod.data$iso_3166))
r2s[[i]]<-data.frame(x=combos[2,i],y=combos[1,i],partial.R2=partial_r2(mod,covariates="unlist(mod.data[, 4])"),df=summary(mod)$df[2])%>%
mutate(`N years`=max(table(mod.data$iso_3166)))
print(r2s[[i]])
print(paste("nrow mod.data=",nrow(mod.data)))
}
}
r2s<-do.call(rbind,r2s)%>%
filter(df>10) ## filter to regressions with at least 10 degrees of freedom
xtable(r2s)
### table S5: predictive ability across input survey questions ####
print.xtable(xtable(r2s,label="tab:timeseries_inputs",
caption="{\\bf Predictive ability across input survey questions:} The table shows the coefficient of determination (Partial R$^2$)
for regressions between responses to each combination of two questions that are asked in three or more years.
Column 3 shows the coefficient of determination after removing country
fixed effects. Column 4 shows the degrees of freedom in the regression, and column 5 shows the number of years for which we have
data from both time series in some overlapping set of countries."),
include.rownames=FALSE,file=paste0(figfolder,"timeseries_inputs",datafilter,"_",modelname,".tex"))
### table S4: predictive ability of our climate concern estimates on standardized, mean responses to question series asked in 3+ years ####
ts.pred<-d.val%>%
nest_by(question)%>%
mutate(mods1=list(lm(yes.scl~mean+factor(iso_3166),data=data)),
partial.r2=partial_r2(mods1,covariates="mean"))%>%
pull(partial.r2,name=question)
ts.pred<-data.frame(Question=names(ts.pred),`Partial R2`=ts.pred)%>% ## some very bad results here for noqfilter_oecd
left_join(discrimination2,by=c("Question"="question"))%>%
left_join(timeseries,by=c("Question"="question"))%>%
select(Question,n,Partial.R2,discrimination)%>%
rename(`Years asked`="n")%>%
arrange(desc(discrimination))
if(exists("datafilter")){
print.xtable(xtable(ts.pred,label="tab:timeseries",
caption="{\\bf Predictive ability of our climate concern estimates on standardized, mean responses to question series asked in 3 or more years: } Column 3 shows the coefficient of determination after removing country fixed effects. Column 4 shows the discrimination parameter estimate, in our climate concern model, for each item."),
include.rownames=FALSE,file=paste0(figfolder,"timeseries_",datafilter,"_",modelname,".tex"))
}else{
print.xtable(xtable(ts.pred,label="tab:timeseries",
caption="{\\bf Predictive ability of our climate concern estimates on standardized, mean responses to question series asked in 3 or more years: } Column 3 shows the coefficient of determination after removing country fixed effects. Column 4 shows the discrimination parameter estimate, in our climate concern model, for each item."),
include.rownames=FALSE,file=paste0(figfolder,"timeseries",datafilter,"_",modelname,".tex"))
}
## aggregated data objects from data that are not publicly shareable
load("inputs/surveys_nonpublic.Rda")
## load survey data and collapse time periods (have to do this anew here b/c survey.data saved in data input only contains questions used...no action or awareness questions) ####
load(paste0("inputs/megapoll_globalmrp_ordinal_replication.Rda"))
survey.data[survey.data=="-1"|survey.data=="-2"|survey.data==-1|survey.data==-2]<-NA
survey.data%<>%filter(as.numeric(year)>=2002)
survey.data<-survey.data%>%
mutate(source2=substr(source,1,nchar(source)-5)) ## take year off so that I can id unique sources that span years
## unique sources including those that span years--assign the last year to the ones that span years, and then merge back in
multi.sources<-survey.data%>%
select(source2,year)%>%
distinct()%>%
mutate(source=grepl("[0-9]+",source2))%>%
group_by(source2,source)%>%
reframe(year2=ifelse(source==TRUE,max(year),NA))%>%
ungroup()%>%
filter(!is.na(year2))%>%
select(source2,year2)%>%
distinct()
survey.data<-left_join(survey.data,multi.sources,by="source2")
survey.data<-survey.data%>%mutate(year2=ifelse(is.na(year2),year,year2))
sort(names(survey.data))
## load survey data and collapse time periods (have to do this anew here b/c survey.data saved in data input only contains questions used...no action or awareness questions) ####
load(paste0("inputs/megapoll_globalmrp_ordinal_replication.Rda"))
names(survey.data)<-gsub("concern","worry",names(survey.data))
names(survey.data)<-gsub("worry_worry","worry",names(survey.data))
names(survey.data)<-gsub("human_caused","attribution",names(survey.data))
names(survey.data)<-gsub("human_cause","attribution",names(survey.data))
names(survey.data)<-gsub("human","attribution",names(survey.data))
names(survey.data)<-gsub("happening_aware_","aware_",names(survey.data))
names(survey.data)<-gsub("happening_aware","aware",names(survey.data))
survey.data2<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry"),mergekey,constructed)%>%
filter_all(any_vars(!is.na(.)))
survey.data%<>%filter(as.numeric(year)>=2002)
survey.data<-left_join(survey.data,multi.sources,by="source2")
## load survey data and collapse time periods (have to do this anew here b/c survey.data saved in data input only contains questions used...no action or awareness questions) ####
load(paste0("inputs/megapoll_globalmrp_ordinal_replication.Rda"))
survey.data%<>%filter(as.numeric(year)>=2002)
survey.data<-left_join(survey.data,multi.sources,by="source2")
survey.data<-survey.data%>%mutate(year2=ifelse(is.na(year2),year,year2))
names(survey.data)<-gsub("worry_worry","worry",names(survey.data))
names(survey.data)<-gsub("human_caused","attribution",names(survey.data))
names(survey.data)<-gsub("human_cause","attribution",names(survey.data))
names(survey.data)<-gsub("human","attribution",names(survey.data))
names(survey.data)<-gsub("happening_aware_","aware_",names(survey.data))
names(survey.data)<-gsub("happening_aware","aware",names(survey.data))
survey.data2<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry"),mergekey,constructed)%>%
filter_all(any_vars(!is.na(.)))
survey.data2<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry"),mergekey)%>%
filter_all(any_vars(!is.na(.)))
## identify number of countries per question: ####
qs<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry_")#,contains("action_"))
)
qs<-names(qs)
d.samples<-survey.data%>%
select(qs,iso_3166)%>%
# mutate_at(contains("concern_"),contains("happening_"),contains("human_"),as.numeric)%>%
group_by(iso_3166)%>%
summarise_at(all_of(qs),funs(n=sum(!is.na(.))))%>%
ungroup()
d.samples<-d.samples%>%
pivot_longer(cols=2:ncol(d.samples),names_to="question",values_to="size")%>%
mutate(question=substr(question,1,nchar(question)-2))
n.qns<-d.samples%>%
group_by(question)%>%
filter(size!=0)%>%
summarize(ctry.n=n_distinct(iso_3166))%>%
arrange(desc(ctry.n))
qlist<-filter(n.qns,ctry.n>=20)
qlist<-unique(qlist$question)
country.yr.means<-list()
for(i in 1:length(qlist)){
print(i)
print(qlist[i])
d<-survey.data%>%
select(iso_3166,year2,qlist[i])%>%
drop_na()%>%
# mutate(qlist[i]=as.numeric(qlist[i]))%>%
group_by(iso_3166,year2)%>%
summarise(val=mean(get(qlist[i])))%>% ## take mean within country-year for this question
# d<-d%>%
group_by(year2)%>% ## group by year and scale
mutate(val=scale(val))%>%
group_by(iso_3166)%>% ## take within-country mean of these scaled values
summarise(val=mean(val,na.rm=TRUE))%>%
mutate(question=qlist[i])
country.yr.means[[i]]<-d
}
sort(names(survey.data))
## load survey data and collapse time periods (have to do this anew here b/c survey.data saved in data input only contains questions used...no action or awareness questions) ####
load(paste0("inputs/megapoll_globalmrp_ordinal_replication.Rda"))
survey.data%<>%filter(as.numeric(year)>=2002)
names(survey.data)<-gsub("concern","worry",names(survey.data))
names(survey.data)<-gsub("worry_worry","worry",names(survey.data))
names(survey.data)<-gsub("human_caused","attribution",names(survey.data))
names(survey.data)<-gsub("human_cause","attribution",names(survey.data))
names(survey.data)<-gsub("human","attribution",names(survey.data))
names(survey.data)<-gsub("happening_aware_","aware_",names(survey.data))
names(survey.data)<-gsub("happening_aware","aware",names(survey.data))
survey.data2<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry"),mergekey)%>%
filter_all(any_vars(!is.na(.)))
## identify number of countries per question: ####
qs<-survey.data%>%
select(contains("attribution"),contains("aware"),contains("worry_")#,contains("action_"))
)
qs<-names(qs)
d.samples<-survey.data%>%
select(qs,iso_3166)%>%
# mutate_at(contains("concern_"),contains("happening_"),contains("human_"),as.numeric)%>%
group_by(iso_3166)%>%
summarise_at(all_of(qs),funs(n=sum(!is.na(.))))%>%
ungroup()
d.samples<-d.samples%>%
pivot_longer(cols=2:ncol(d.samples),names_to="question",values_to="size")%>%
mutate(question=substr(question,1,nchar(question)-2))
n.qns<-d.samples%>%
group_by(question)%>%
filter(size!=0)%>%
summarize(ctry.n=n_distinct(iso_3166))%>%
arrange(desc(ctry.n))
qlist<-filter(n.qns,ctry.n>=20)
qlist<-unique(qlist$question)
country.yr.means<-list()
for(i in 1:length(qlist)){
print(i)
print(qlist[i])
d<-survey.data%>%
select(iso_3166,year2,qlist[i])%>%
drop_na()%>%
# mutate(qlist[i]=as.numeric(qlist[i]))%>%
group_by(iso_3166,year2)%>%
summarise(val=mean(get(qlist[i])))%>% ## take mean within country-year for this question
# d<-d%>%
group_by(year2)%>% ## group by year and scale
mutate(val=scale(val))%>%
group_by(iso_3166)%>% ## take within-country mean of these scaled values
summarise(val=mean(val,na.rm=TRUE))%>%
mutate(question=qlist[i])
country.yr.means[[i]]<-d
}
country.yr.means<-do.call(rbind,country.yr.means)
country.yr.means%<>%bind_rows(country.yr.means.np)
country.yr.means.wide<-pivot_wider(country.yr.means,id_cols=c(iso_3166),names_from="question",values_from="val")
cormat.yr<-cor(country.yr.means.wide[,3:ncol(country.yr.means.wide)],use="pairwise.complete.obs")
## find average correlation for each variable
cormat.df<-data.frame(cormat.yr)
avgcor<-apply(cormat.df,MARGIN = 2,mean,na.rm=TRUE)
avgcor<-data.frame(var=names(cormat.df),cor=round(avgcor,2))%>%
mutate(cor=paste(var,cor,sep=": "))
rownames(cormat.yr)<-avgcor$cor
colnames(cormat.yr)<-avgcor$cor
quartz(12,12)
corrplot(cormat.yr,order="alphabet",
#order="FPC",## order by first principal component
tl.pos="lt",
tl.cex=.5,number.cex=0.5)
library(nFactors)
corrplot(cormat.yr,order="alphabet",
#order="FPC",## order by first principal component
tl.pos="lt",
tl.cex=.5,number.cex=0.5)
library(reshape2)
library(countrycode)
library(ggplot2)
library(ggrepel)
library(broom)
library(ggforce)
library(ggpubr)
library(corrplot)
# library(ggsflabel)
library(rmapshaper)
library(sp)
# devtools::install_github("ianmoran11/mmtable2")
library(mmtable2)
library(xtable)
library(magrittr)
library(gt)
library(RColorBrewer)
library(colorspace)
library(nFactors)
library(estimatr)
library(readxl)
library(sf)
library(fixest)
library(grid) ## for function "unit" in maps
library(ggspatial) ## for north arrow function in maps
library(tidyverse)
corrplot(cormat.yr,order="alphabet",
#order="FPC",## order by first principal component
tl.pos="lt",
tl.cex=.5,number.cex=0.5)
pdf(paste0(figfolder,"corplot_country_year_sub.pdf"))
corrplot(cormat.yr,order="alphabet",
#order="FPC",## order by first principal component
tl.pos="lt",
tl.cex=.5,number.cex=0.5)
dev.off()
## aggregated data objects from data that are not publicly shareable
load("inputs/surveys_nonpublic.Rda")
## aggregated data objects from data that are not publicly shareable
load("inputs/surveys_nonpublic.Rda")
## how many *respondents* are in the dataset.
respondents<-survey.data%>%select(all_of(qs$question))%>%
filter_all(any_vars(!is.na(.))) ## This will be an under-count in replication code because of non-shareable data; but add to nrow(constructed to get full count)
# Preamble ####
rm(list=ls())
library(reshape2)
library(countrycode)
library(ggplot2)
library(ggrepel)
library(broom)
library(ggforce)
library(ggpubr)
library(corrplot)
# library(ggsflabel)
library(rmapshaper)
library(sp)
# devtools::install_github("ianmoran11/mmtable2")
library(mmtable2)
library(xtable)
library(magrittr)
library(gt)
library(RColorBrewer)
library(colorspace)
library(nFactors)
library(estimatr)
library(readxl)
library(sf)
library(fixest)
library(grid) ## for function "unit" in maps
library(ggspatial) ## for north arrow function in maps
library(tidyverse)
# source("globalmrp_functions.R")
onUser<-function(x){
user<-Sys.info()["user"]
onD<-grepl(x,user,ignore.case=TRUE)
return(onD)
}
if(onUser("pberg")){
setwd("~/Documents/GitHub/climateconcern")
figfolder<-"~/Documents/GitHub/climateconcern/figures/"
}
## Clara set up your folders here
if(onUser("clara")){
setwd("~/Documents/climateconcern")
figfolder<-"~/Documents/climateconcern/figures/"
}
#load cleaned model outputs
modelname<-"country_walk_region_walk_fxdstart" ## new model 240715
timecollapse<-"2yr"
qrestrict<-"concernhuman"
datafilter<-paste(timecollapse,qrestrict,sep="_")
if(exists("datafilter")){
load(paste0("outputs_stan/stan_clean_",modelname,"_",datafilter,".Rdata"))
}else{
load(paste0("outputs_stan/stan_clean_",modelname,".Rdata"))
}
## aggregated data objects from data that are not publicly shareable
load("inputs/surveys_nonpublic.Rda")
## number of questions in our final model:
length(unique(d$question)) #78
length(unique(d.nat$question)) # 64
## total sample
sum(d$yes) ## 3.6 million
sum(d.nat$yes) ## 301578
qs<-data.frame(question=c(unique(d$question),unique(d.nat$question)))%>%distinct()
nrow(qs) ##81
## how many *respondents* are in the dataset.
respondents<-survey.data%>%select(all_of(qs$question))%>%
filter_all(any_vars(!is.na(.))) ## This will be an under-count in replication code because of non-shareable data; but add to nrow(constructed to get full count)
nrow(nonpublic.constructed)
nrow(nonpublic.constructed) + nrow(respondents)
